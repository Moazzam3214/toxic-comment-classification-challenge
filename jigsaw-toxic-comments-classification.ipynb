{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2798066,"sourceType":"datasetVersion","datasetId":1709138}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Toxic Comment Classification Challenge\n\nThis notebook analyzes and builds machine learning models for the Jigsaw Toxic Comment Classification Challenge. The goal is to identify and classify toxic online comments into different categories of toxicity.\n\n## Dataset Overview\nThe dataset contains comments with the following toxicity labels:\n- `toxic`: General toxicity\n- `severe_toxic`: Severely toxic comments\n- `obscene`: Obscene language\n- `threat`: Threatening comments\n- `insult`: Insulting comments\n- `identity_hate`: Identity-based hate speech","metadata":{"_uuid":"ff917e97-fc42-404c-9548-d53345be079a","_cell_guid":"f79ed02b-4138-4e53-9491-aa6270c91801","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"31cf019d-a186-4f7f-99df-78438687e1fe","_cell_guid":"9cd5959e-c9d0-4d0a-a53c-8f511bf73c72","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:24.221834Z","iopub.execute_input":"2025-05-30T06:59:24.222771Z","iopub.status.idle":"2025-05-30T06:59:24.735857Z","shell.execute_reply.started":"2025-05-30T06:59:24.222742Z","shell.execute_reply":"2025-05-30T06:59:24.734966Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Import Required Libraries\n\nSetting up all necessary libraries for data processing, text cleaning, and machine learning.","metadata":{"_uuid":"b45baa34-4f33-49d3-ab24-c068e6ac6014","_cell_guid":"43f1e596-d777-4946-875d-2b14d82675df","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport warnings\n\n# Machine Learning Libraries\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\n\n# Text Processing Libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n# Configuration\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth = 300\npd.options.display.max_columns = 100","metadata":{"_uuid":"8fb8c875-4297-48b2-8e68-360b07083b96","_cell_guid":"3556069b-8b74-4128-a4d3-dc8f160d4fdc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:24.736516Z","iopub.execute_input":"2025-05-30T06:59:24.736932Z","iopub.status.idle":"2025-05-30T06:59:24.751781Z","shell.execute_reply.started":"2025-05-30T06:59:24.736907Z","shell.execute_reply":"2025-05-30T06:59:24.750983Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Loading and Initial Exploration\n\nLoading the training and test datasets and performing initial data exploration to understand the structure and characteristics of the data.","metadata":{"_uuid":"43785c53-f213-4b11-99d7-90e543810e83","_cell_guid":"17878300-bd28-4ba9-b09d-4780039d2169","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load datasets\ndf_train = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n\nprint(f\"Training data shape: {df_train.shape}\")\nprint(f\"Test data shape: {df_test.shape}\")","metadata":{"_uuid":"7d9e323d-bccb-4224-ae7b-02c932b00e1c","_cell_guid":"89ad7249-aeb9-4219-81bc-54e3f2b34ee2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:24.755375Z","iopub.execute_input":"2025-05-30T06:59:24.755678Z","iopub.status.idle":"2025-05-30T06:59:28.720360Z","shell.execute_reply.started":"2025-05-30T06:59:24.755653Z","shell.execute_reply":"2025-05-30T06:59:28.719646Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine datasets for unified preprocessing\ndf_train['is_train'] = 1\ndf_test['is_train'] = 0\n\ndf = pd.concat([df_train, df_test], ignore_index=True)","metadata":{"_uuid":"3c7013a2-6432-4523-9a10-7b32b9fa3a0b","_cell_guid":"6663ed02-6aeb-41f7-a269-69d3884d9438","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:28.721212Z","iopub.execute_input":"2025-05-30T06:59:28.721428Z","iopub.status.idle":"2025-05-30T06:59:28.773533Z","shell.execute_reply.started":"2025-05-30T06:59:28.721405Z","shell.execute_reply":"2025-05-30T06:59:28.772831Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display combined dataset\ndf","metadata":{"_uuid":"57996853-99de-4371-9e34-8d794b47c623","_cell_guid":"e3738623-57c1-4720-8039-ffcc455885a3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:28.774466Z","iopub.execute_input":"2025-05-30T06:59:28.774772Z","iopub.status.idle":"2025-05-30T06:59:28.810692Z","shell.execute_reply.started":"2025-05-30T06:59:28.774744Z","shell.execute_reply":"2025-05-30T06:59:28.809939Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Quality Assessment\n\nChecking for missing values, data types, and overall data quality to identify any preprocessing needs.","metadata":{"_uuid":"b96650bf-73dd-4fb9-b850-e407afd61a8e","_cell_guid":"3a925cca-8c66-4902-9d35-b72e8e93e7a9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Check for missing values\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())","metadata":{"_uuid":"88760a72-3132-4857-9fbc-6604b3defadc","_cell_guid":"e4b7828c-6c6e-4971-a72a-fd510581fd84","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:28.811377Z","iopub.execute_input":"2025-05-30T06:59:28.811710Z","iopub.status.idle":"2025-05-30T06:59:28.892946Z","shell.execute_reply.started":"2025-05-30T06:59:28.811689Z","shell.execute_reply":"2025-05-30T06:59:28.892224Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset information\ndf.info()","metadata":{"_uuid":"54410f12-4bc9-48b5-ad68-5a71ed078696","_cell_guid":"f88c3952-73a0-4bc9-bb71-2bd1f289100f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:28.893747Z","iopub.execute_input":"2025-05-30T06:59:28.893994Z","iopub.status.idle":"2025-05-30T06:59:28.982693Z","shell.execute_reply.started":"2025-05-30T06:59:28.893976Z","shell.execute_reply":"2025-05-30T06:59:28.981946Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Statistical summary\ndf.describe()","metadata":{"_uuid":"b1d5745b-70db-4f77-a8fe-6f02e5e89018","_cell_guid":"a7768145-3b7a-4c61-94ee-f9061afd1366","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:28.983449Z","iopub.execute_input":"2025-05-30T06:59:28.983672Z","iopub.status.idle":"2025-05-30T06:59:29.064545Z","shell.execute_reply.started":"2025-05-30T06:59:28.983654Z","shell.execute_reply":"2025-05-30T06:59:29.063835Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for duplicates\nprint(f\"Number of duplicate rows: {df.duplicated().sum()}\")","metadata":{"_uuid":"a4434da8-009d-4192-a8b1-b44e6f2c057d","_cell_guid":"77118634-f65b-4d10-85f0-6e98082cfaac","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:29.067217Z","iopub.execute_input":"2025-05-30T06:59:29.067465Z","iopub.status.idle":"2025-05-30T06:59:29.157136Z","shell.execute_reply.started":"2025-05-30T06:59:29.067444Z","shell.execute_reply":"2025-05-30T06:59:29.156347Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Exploratory Data Analysis\n\nExamining the distribution of toxicity labels and exploring sample comments for each category to better understand the data characteristics.","metadata":{"_uuid":"9a5707d6-3381-4f36-9d70-1ffb96568a78","_cell_guid":"0f5c58cf-b14d-4973-bc69-d8d322b5b493","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Examine comment text column\nprint(\"Sample comment texts:\")\ndf[\"comment_text\"].head()","metadata":{"_uuid":"d77185a0-2066-4c71-8636-8c8aa754dacc","_cell_guid":"d0c40c71-cea3-413c-aca1-b6348263664d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:29.157922Z","iopub.execute_input":"2025-05-30T06:59:29.158150Z","iopub.status.idle":"2025-05-30T06:59:29.165257Z","shell.execute_reply.started":"2025-05-30T06:59:29.158133Z","shell.execute_reply":"2025-05-30T06:59:29.164396Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explore toxicity categories with sample comments\ntoxicity_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nfor col in toxicity_columns:\n    print(f'****** {col.upper()} EXAMPLES *******')\n    display(df.loc[df[col]==1,['comment_text',col]].sample(5))","metadata":{"_uuid":"276be481-cf6d-4dbf-b924-6c4063e3ac64","_cell_guid":"94d0bdab-e42b-4dbd-8759-3b228fa2b842","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:29.881552Z","iopub.execute_input":"2025-05-30T06:59:29.881799Z","iopub.status.idle":"2025-05-30T06:59:29.930313Z","shell.execute_reply.started":"2025-05-30T06:59:29.881781Z","shell.execute_reply":"2025-05-30T06:59:29.929520Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a 'clean' label for non-toxic comments\ndf['clean'] = (df_train[toxicity_columns].sum(axis=1) == 0).astype(int)\nprint(\"Dataset with clean label:\")\ndf.head()","metadata":{"_uuid":"ee8e9beb-742b-4557-a437-7c5d8aa2ce0c","_cell_guid":"9e5560ac-968e-4ef1-b530-9d531f2f1805","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:29.931099Z","iopub.execute_input":"2025-05-30T06:59:29.931298Z","iopub.status.idle":"2025-05-30T06:59:29.930313Z","shell.execute_reply.started":"2025-05-30T06:59:29.931282Z","shell.execute_reply":"2025-05-30T06:59:29.929520Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Text Preprocessing and Cleaning\n\nImplementing comprehensive text cleaning including URL removal, HTML tag removal, stopword removal, and stemming to prepare the text data for machine learning models.","metadata":{"_uuid":"871c1899-b7e6-4c92-bf5d-0c6c07529c1a","_cell_guid":"0c8fb636-dfab-4800-9560-ebfceef78f62","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Download NLTK resources\nnltk.download('stopwords')\n\n# Initialize text processing tools\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\n\ndef clean_text(text):\n    \"\"\"\n    Comprehensive text cleaning function\n    \n    Steps:\n    1. Convert to lowercase\n    2. Remove newlines, URLs, and HTML tags\n    3. Keep only alphabetic characters\n    4. Remove extra spaces\n    5. Remove stopwords and apply stemming\n    \"\"\"\n    text = str(text).lower()\n    text = re.sub(r'\\n', ' ', text)                    # Remove newlines\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n    text = re.sub(r'<.*?>', '', text)                  # Remove HTML tags\n    text = re.sub(r'[^a-z\\s]', '', text)               # Keep only letters and spaces\n    text = re.sub(r'\\s+', ' ', text).strip()           # Remove extra spaces\n\n    # Tokenize, remove stopwords, and stem\n    words = text.split()\n    cleaned_words = []\n    for w in words:\n        if w and w not in stop_words:\n            try:\n                stemmed = stemmer.stem(w)\n                cleaned_words.append(stemmed)\n            except RecursionError:\n                pass  # Skip words causing stemmer errors\n\n    return ' '.join(cleaned_words)\n\n# Apply text cleaning to all comments\nprint(\"Applying text cleaning... This may take a few minutes.\")\ndf['comment_text_clean'] = df['comment_text'].apply(clean_text)\nprint(\"Text cleaning completed!\")","metadata":{"_uuid":"148cf99f-0dac-464b-83b4-9daa5bb474f1","_cell_guid":"da3ee760-53aa-4705-82d1-693b1ecfd207","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T06:59:29.931099Z","iopub.execute_input":"2025-05-30T06:59:29.931298Z","iopub.status.idle":"2025-05-30T07:02:31.135221Z","shell.execute_reply.started":"2025-05-30T06:59:29.931282Z","shell.execute_reply":"2025-05-30T07:02:31.134471Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare original and cleaned text\nprint(\"Original vs Cleaned Text Comparison:\")\ndf[['comment_text', 'comment_text_clean']].head()","metadata":{"_uuid":"c551cf04-b54e-4a32-b205-c5c99b17a4f0","_cell_guid":"41a69d49-05a9-4998-84c2-75f5287fba69","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:02:31.136074Z","iopub.execute_input":"2025-05-30T07:02:31.136450Z","iopub.status.idle":"2025-05-30T07:02:31.169270Z","shell.execute_reply.started":"2025-05-30T07:02:31.136431Z","shell.execute_reply":"2025-05-30T07:02:31.168346Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Data Preparation for Machine Learning\n\nSplitting the data back into training and test sets, and preparing features using TF-IDF vectorization for model training.","metadata":{"_uuid":"424ddb14-cf63-407b-81cc-6221889f2218","_cell_guid":"5571cf14-6969-4de7-bb6c-1311abbe1069","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Split back into train and test sets\ntrain_df = df[df['is_train'] == 1].copy()\ntest_df = df[df['is_train'] == 0].copy()\n\n# Clean up the datasets\ntrain_df.drop(\"is_train\", inplace=True, axis=1)\ntest_df.drop(\"is_train\", inplace=True, axis=1)\n\nprint(f\"Final training set shape: {train_df.shape}\")\nprint(f\"Final test set shape: {test_df.shape}\")","metadata":{"_uuid":"3c9815fb-2399-4c28-9c79-e76db3e4fae6","_cell_guid":"38a99e9e-0320-4326-a78e-40b5cd9b16df","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:02:31.170109Z","iopub.execute_input":"2025-05-30T07:02:31.170380Z","iopub.status.idle":"2025-05-30T07:02:31.274574Z","shell.execute_reply.started":"2025-05-30T07:02:31.170354Z","shell.execute_reply":"2025-05-30T07:02:31.273962Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display training data structure\ntrain_df.head()","metadata":{"_uuid":"4bff9f44-a721-47d5-8bb8-e04b1c2d1fa3","_cell_guid":"1f217a06-379e-41c1-8ccf-edab770fa51d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:02:31.275328Z","iopub.execute_input":"2025-05-30T07:02:31.275650Z","iopub.status.idle":"2025-05-30T07:02:31.294156Z","shell.execute_reply.started":"2025-05-30T07:02:31.275625Z","shell.execute_reply":"2025-05-30T07:02:31.293345Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display test data structure\ntest_df.head()","metadata":{"_uuid":"baf67c39-caf9-460e-9e7f-b97eae077262","_cell_guid":"cd989aad-f2a3-4457-a27b-fac1ba1b3bd7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:02:31.294928Z","iopub.execute_input":"2025-05-30T07:02:31.295196Z","iopub.status.idle":"2025-05-30T07:02:31.313306Z","shell.execute_reply.started":"2025-05-30T07:02:31.295168Z","shell.execute_reply":"2025-05-30T07:02:31.312496Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Feature Engineering with TF-IDF\n\nCreating numerical features from text using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization with optimized parameters for better model performance.","metadata":{"_uuid":"833cf8fb-c462-4385-9c05-736752d1a83b","_cell_guid":"4a9049cf-9fce-4532-942f-cb9cb54d8c4a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Initialize TF-IDF Vectorizer with optimized parameters\ntfidf = TfidfVectorizer(\n    max_features=10000,        # Capture more useful words\n    ngram_range=(1, 2),        # Use unigrams and bigrams\n    min_df=3,                  # Ignore rare terms (appear in less than 3 documents)\n    max_df=0.9,                # Ignore very common terms (appear in more than 90% of documents)\n    strip_accents='unicode',   # Handle accented characters\n    sublinear_tf=True          # Use sublinear term frequency scaling\n)\n\nprint(\"TF-IDF Vectorizer configured with parameters:\")\nprint(f\"- Max features: {tfidf.max_features}\")\nprint(f\"- N-gram range: {tfidf.ngram_range}\")\nprint(f\"- Min document frequency: {tfidf.min_df}\")\nprint(f\"- Max document frequency: {tfidf.max_df}\")","metadata":{"_uuid":"b61c9248-efeb-4e0a-beeb-932236503464","_cell_guid":"fcb71101-afb2-4d0e-8381-58c4c61cbf4b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:02:31.314082Z","iopub.execute_input":"2025-05-30T07:02:31.314341Z","iopub.status.idle":"2025-05-30T07:02:31.330131Z","shell.execute_reply.started":"2025-05-30T07:02:31.314323Z","shell.execute_reply":"2025-05-30T07:02:31.329282Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transform text to numerical features\nprint(\"Transforming text data to TF-IDF features...\")\nX_train = tfidf.fit_transform(train_df['comment_text_clean'])\n\n# Prepare target labels (all toxicity categories + clean label)\ny_train = train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'clean']].values\n\nprint(f\"Feature matrix shape: {X_train.shape}\")\nprint(f\"Label matrix shape: {y_train.shape}\")\nprint(\"Feature engineering completed!\")","metadata":{"_uuid":"9aa8cb44-4a0c-489c-b24b-9fbfdc355338","_cell_guid":"2c992e68-8bc9-429b-9bfb-cb27604b7175","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:02:31.331124Z","iopub.execute_input":"2025-05-30T07:02:31.331438Z","iopub.status.idle":"2025-05-30T07:02:52.795466Z","shell.execute_reply.started":"2025-05-30T07:02:31.331393Z","shell.execute_reply":"2025-05-30T07:02:52.794637Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the prepared data\nprint(\"Training features and labels ready:\")\nprint(f\"X_train type: {type(X_train)}\")\nprint(f\"y_train type: {type(y_train)}\")","metadata":{"_uuid":"d8541789-6802-4b6f-ae40-daeb5bd4cd9f","_cell_guid":"cc127218-82f6-4190-a00a-5c5ebc6c8241","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:02:52.796364Z","iopub.execute_input":"2025-05-30T07:02:52.796696Z","iopub.status.idle":"2025-05-30T07:02:52.803294Z","shell.execute_reply.started":"2025-05-30T07:02:52.796669Z","shell.execute_reply":"2025-05-30T07:02:52.802541Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Model Training and Evaluation\n\nTraining multiple machine learning models and comparing their performance using F1-score. We use OneVsRestClassifier to handle the multi-label classification problem.","metadata":{"_uuid":"60847459-82a2-4746-9f55-c475c119ae56","_cell_guid":"a8cb0d30-9ede-4438-943b-2fe41051ba3e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Define models for comparison\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Random Forest': RandomForestClassifier(n_estimators=100, n_jobs=-1),\n    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n}\n\nprint(\"Training models and evaluating performance...\")\nprint(\"=\" * 50)\n\nresults = {}\n\n# Train and evaluate each model\nfor name, base_model in models.items():\n    print(f\"\\nTraining {name}...\")\n    \n    # Use OneVsRestClassifier for multi-label classification\n    clf = OneVsRestClassifier(base_model)\n    clf.fit(X_train, y_train)\n    \n    # Make predictions and calculate F1-score\n    y_pred = clf.predict(X_train)\n    f1 = f1_score(y_train, y_pred, average='macro')\n    results[name] = f1\n    \n    print(f\"{name}: F1-score (train) = {f1:.4f}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Training completed!\")","metadata":{"_uuid":"04df5cd5-22af-47af-b468-d43cb3707c15","_cell_guid":"0b2ebbd9-b904-4caa-9f3b-a1096fc86074","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:02:52.804285Z","iopub.execute_input":"2025-05-30T07:02:52.804615Z","iopub.status.idle":"2025-05-30T07:23:52.396405Z","shell.execute_reply.started":"2025-05-30T07:02:52.804571Z","shell.execute_reply":"2025-05-30T07:23:52.395632Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Cross-Validation Evaluation\n\nPerforming cross-validation to get a more robust estimate of model performance and avoid overfitting.","metadata":{"_uuid":"c730c063-9034-476f-9be5-0199971335dd","_cell_guid":"a8a1a47e-2bb4-451e-9d8f-0cdc97fc436e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Performing 3-fold cross-validation...\")\nprint(\"=\" * 50)\n\ncv_results = {}\n\n# Perform cross-validation for each model\nfor name, base_model in models.items():\n    print(f\"\\nCross-validating {name}...\")\n    \n    clf = OneVsRestClassifier(base_model)\n    scores = cross_val_score(clf, X_train, y_train, scoring='f1_macro', cv=3)\n    \n    cv_results[name] = {\n        'mean': scores.mean(),\n        'std': scores.std(),\n        'scores': scores\n    }\n    \n    print(f\"{name}: CV F1-score = {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Cross-validation completed!\")","metadata":{"_uuid":"d631f120-ff21-4fa0-a14b-2a43b0380867","_cell_guid":"d53177f9-304c-473c-ba9f-eee0910ee82c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-30T07:27:51.659248Z","iopub.execute_input":"2025-05-30T07:27:51.659606Z","iopub.status.idle":"2025-05-30T08:03:03.173474Z","shell.execute_reply.started":"2025-05-30T07:27:51.659555Z","shell.execute_reply":"2025-05-30T08:03:03.172714Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Results Summary\n\nSummary of model performance and recommendations for next steps.","metadata":{"_uuid":"7de489a9-36dd-4937-9fe6-8ccfe1634e44","_cell_guid":"9fb6189f-e83d-456a-a52e-fc4a511d2783","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Display final results summary\nprint(\"FINAL RESULTS SUMMARY\")\nprint(\"=\" * 60)\nprint()\n\nprint(\"Training F1-Scores:\")\nprint(\"-\" * 30)\nfor name, score in results.items():\n    print(f\"{name:<20}: {score:.4f}\")\n\nprint()\nprint(\"Cross-Validation F1-Scores:\")\nprint(\"-\" * 35)\nfor name, result in cv_results.items():\n    print(f\"{name:<20}: {result['mean']:.4f} (+/- {result['std'] * 2:.4f})\")\n\nprint()\nprint(\"RECOMMENDATIONS:\")\nprint(\"-\" * 20)\nbest_model = max(cv_results.keys(), key=lambda x: cv_results[x]['mean'])\nprint(f\"• Best performing model: {best_model}\")\nprint(f\"• Best CV F1-score: {cv_results[best_model]['mean']:.4f}\")\nprint()\nprint(\"NEXT STEPS:\")\nprint(\"• Hyperparameter tuning for the best model\")\nprint(\"• Feature engineering improvements\")\nprint(\"• Ensemble methods combining multiple models\")\nprint(\"• Generate predictions for test set\")","metadata":{"_uuid":"a6c44e4e-6088-4385-bf09-dac577a2d0ea","_cell_guid":"e0bcba71-45d4-44e2-8864-71f51618da9a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nThis notebook successfully implemented a multi-label text classification pipeline for toxic comment detection. The approach included:\n\n1. **Data Preprocessing**: Comprehensive text cleaning with URL removal, HTML stripping, and linguistic preprocessing\n2. **Feature Engineering**: TF-IDF vectorization with optimized parameters for text representation\n3. **Model Comparison**: Evaluation of multiple machine learning algorithms using OneVsRestClassifier\n4. **Performance Evaluation**: Both training and cross-validation metrics to assess model reliability\n\nThe results provide a solid baseline for toxic comment classification, with opportunities for further improvement through hyperparameter tuning and advanced feature engineering techniques.","metadata":{"_uuid":"a9eac41f-ee7f-44f6-ae1c-04e0b87f56ac","_cell_guid":"0ab00f45-f45c-49a9-8fcf-a9338c2427a5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}